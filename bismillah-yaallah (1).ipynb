{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8673578,"sourceType":"datasetVersion","datasetId":5198606}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. import Libarary \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-19T13:40:13.682377Z","iopub.execute_input":"2024-06-19T13:40:13.682675Z","iopub.status.idle":"2024-06-19T13:40:26.030206Z","shell.execute_reply.started":"2024-06-19T13:40:13.682647Z","shell.execute_reply":"2024-06-19T13:40:26.029445Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-19 13:40:15.525787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-19 13:40:15.525924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-19 13:40:15.663787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. Persiapan Dataset\n# Menggunakan ImageDataGenerator untuk mempersiapkan dataset dan augmentasi gambar\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2  # memisahkan dataset menjadi training dan validation set\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/capstone-bangkit-sampah-organik-and-anorganik/DATASET/TRAIN',  \n    target_size=(150, 150),  # ukuran gambar\n    batch_size=32,\n    class_mode='binary',  # gunakan 'binary' jika hanya ada dua kelas\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/capstone-bangkit-sampah-organik-and-anorganik/DATASET/TEST',\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T13:40:33.076073Z","iopub.execute_input":"2024-06-19T13:40:33.076693Z","iopub.status.idle":"2024-06-19T13:40:45.442567Z","shell.execute_reply.started":"2024-06-19T13:40:33.076661Z","shell.execute_reply":"2024-06-19T13:40:45.441707Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 18052 images belonging to 2 classes.\nFound 502 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# 3. Membangun Model CNN\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T13:40:57.342170Z","iopub.execute_input":"2024-06-19T13:40:57.342546Z","iopub.status.idle":"2024-06-19T13:40:58.177734Z","shell.execute_reply.started":"2024-06-19T13:40:57.342519Z","shell.execute_reply":"2024-06-19T13:40:58.176975Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 4. Kompilasi Model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T13:41:05.322251Z","iopub.execute_input":"2024-06-19T13:41:05.323256Z","iopub.status.idle":"2024-06-19T13:41:05.333041Z","shell.execute_reply.started":"2024-06-19T13:41:05.323214Z","shell.execute_reply":"2024-06-19T13:41:05.332133Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 5. Pelatihan Model\ncheckpoint = ModelCheckpoint('model.keras', monitor='val_loss', save_best_only=True, mode='min')\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // validation_generator.batch_size,\n    epochs=20,\n    callbacks=[checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T13:41:09.264746Z","iopub.execute_input":"2024-06-19T13:41:09.265104Z","iopub.status.idle":"2024-06-19T14:00:03.959356Z","shell.execute_reply.started":"2024-06-19T13:41:09.265075Z","shell.execute_reply":"2024-06-19T14:00:03.958561Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/564\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43:53\u001b[0m 11s/step - accuracy: 0.3750 - loss: 0.7221","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718804480.868086     120 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1718804480.888870     120 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m144/564\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 263ms/step - accuracy: 0.6838 - loss: 0.7255","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718804518.571745     119 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.7595 - loss: 0.5510","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718804619.743834     118 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 254ms/step - accuracy: 0.7596 - loss: 0.5508 - val_accuracy: 0.9458 - val_loss: 0.1971\nEpoch 2/20\n\u001b[1m  1/564\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 32ms/step - accuracy: 0.8125 - loss: 0.4607","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.4607 - val_accuracy: 0.9545 - val_loss: 0.2090\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718804625.295963     119 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 189ms/step - accuracy: 0.8302 - loss: 0.3947 - val_accuracy: 0.9458 - val_loss: 0.1797\nEpoch 4/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8438 - loss: 0.4408 - val_accuracy: 0.9545 - val_loss: 0.1248\nEpoch 5/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 196ms/step - accuracy: 0.8440 - loss: 0.3686 - val_accuracy: 0.9333 - val_loss: 0.1759\nEpoch 6/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192us/step - accuracy: 0.8125 - loss: 0.3090 - val_accuracy: 0.8636 - val_loss: 0.2268\nEpoch 7/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 198ms/step - accuracy: 0.8606 - loss: 0.3409 - val_accuracy: 0.9167 - val_loss: 0.1751\nEpoch 8/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4787 - val_accuracy: 1.0000 - val_loss: 0.1025\nEpoch 9/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 188ms/step - accuracy: 0.8611 - loss: 0.3284 - val_accuracy: 0.9542 - val_loss: 0.1317\nEpoch 10/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201us/step - accuracy: 0.8750 - loss: 0.2650 - val_accuracy: 0.9545 - val_loss: 0.2022\nEpoch 11/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 187ms/step - accuracy: 0.8706 - loss: 0.3232 - val_accuracy: 0.9375 - val_loss: 0.1723\nEpoch 12/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.3371 - val_accuracy: 0.9545 - val_loss: 0.0745\nEpoch 13/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 186ms/step - accuracy: 0.8778 - loss: 0.3041 - val_accuracy: 0.9375 - val_loss: 0.1592\nEpoch 14/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155us/step - accuracy: 0.9375 - loss: 0.3326 - val_accuracy: 0.9091 - val_loss: 0.2103\nEpoch 15/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 189ms/step - accuracy: 0.8814 - loss: 0.3029 - val_accuracy: 0.9604 - val_loss: 0.1123\nEpoch 16/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178us/step - accuracy: 0.9062 - loss: 0.2217 - val_accuracy: 0.9091 - val_loss: 0.1317\nEpoch 17/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 189ms/step - accuracy: 0.8815 - loss: 0.2906 - val_accuracy: 0.9646 - val_loss: 0.1191\nEpoch 18/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173us/step - accuracy: 0.9688 - loss: 0.2178 - val_accuracy: 0.9545 - val_loss: 0.2406\nEpoch 19/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 184ms/step - accuracy: 0.8857 - loss: 0.2821 - val_accuracy: 0.9417 - val_loss: 0.1729\nEpoch 20/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - accuracy: 0.9375 - loss: 0.1933 - val_accuracy: 1.0000 - val_loss: 0.1124\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\n# 1. Memuat model .keras\nmodel_path = 'model.keras'\nmodel = tf.keras.models.load_model(model_path)\nprint(\"Model berhasil dimuat.\")\n\n# 2. Fungsi untuk memuat dan memproses gambar\ndef load_and_preprocess_image(image_path, target_size):\n    img = Image.open(image_path).resize(target_size)\n    img = np.array(img, dtype=np.float32)\n    img = img / 255.0  # Normalisasi gambar\n    img = np.expand_dims(img, axis=0)  # Tambahkan batch dimension\n    return img\n\n# Path ke gambar input untuk diuji\nimage_path = '/kaggle/input/capstone-bangkit-sampah-organik-and-anorganik/DATASET/TEST/A/R_10040.jpg'  # Ganti dengan path ke gambar uji Anda\ninput_image = load_and_preprocess_image(image_path, target_size=(150, 150))\n\n# 3. Menjalankan prediksi menggunakan model\npredictions = model.predict(input_image)\nprint(\"Prediksi:\", predictions)\n\n# 4. Fungsi untuk menampilkan hasil prediksi\ndef display_prediction(prediction):\n    if prediction >= 0.5:\n        print(\"Prediksi: Organik\")\n    else:\n        print(\"Prediksi: Anorganik\")\n\n# Menampilkan hasil prediksi\ndisplay_prediction(predictions[0][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:15:08.346450Z","iopub.execute_input":"2024-06-19T14:15:08.346985Z","iopub.status.idle":"2024-06-19T14:15:11.168822Z","shell.execute_reply.started":"2024-06-19T14:15:08.346944Z","shell.execute_reply":"2024-06-19T14:15:11.167910Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model berhasil dimuat.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\nPrediksi: [[0.39233056]]\nPrediksi: Anorganik\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:15:21.983052Z","iopub.execute_input":"2024-06-19T14:15:21.983420Z","iopub.status.idle":"2024-06-19T14:15:21.987794Z","shell.execute_reply.started":"2024-06-19T14:15:21.983392Z","shell.execute_reply":"2024-06-19T14:15:21.986896Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"! pip install -q tensorflow\n! pip install -q tensorflow-model-optimization\n\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow_model_optimization as tfmot\nimport tf_keras as keras\n\nimport tempfile\n\ninput_shape = [20]\nx_train = np.random.randn(1, 20).astype(np.float32)\ny_train = keras.utils.to_categorical(np.random.randn(1), num_classes=20)\n\ndef setup_model():\n  model = keras.Sequential([\n      keras.layers.Dense(20, input_shape=input_shape),\n      keras.layers.Flatten()\n  ])\n  return model\n\ndef setup_pretrained_weights():\n  model= setup_model()\n\n  model.compile(\n      loss=keras.losses.categorical_crossentropy,\n      optimizer='adam',\n      metrics=['accuracy']\n  )\n\n  model.fit(x_train, y_train)\n\n  _, pretrained_weights = tempfile.mkstemp('.tf')\n\n  model.save_weights(pretrained_weights)\n\n  return pretrained_weights\n\ndef setup_pretrained_model():\n  model = setup_model()\n  pretrained_weights = setup_pretrained_weights()\n  model.load_weights(pretrained_weights)\n  return model\n\nsetup_model()\npretrained_weights = setup_pretrained_weights()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:16:15.586800Z","iopub.execute_input":"2024-06-19T14:16:15.587799Z","iopub.status.idle":"2024-06-19T14:17:11.062862Z","shell.execute_reply.started":"2024-06-19T14:16:15.587761Z","shell.execute_reply":"2024-06-19T14:17:11.061849Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2024-06-19 14:16:44.607602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-19 14:16:44.607665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-19 14:16:44.609203: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"1/1 [==============================] - 21s 21s/step - loss: 16.1181 - accuracy: 0.0000e+00\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718806630.880457     439 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = setup_model()\nbase_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n\nquant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\nquant_aware_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:17:34.446058Z","iopub.execute_input":"2024-06-19T14:17:34.446946Z","iopub.status.idle":"2024-06-19T14:17:37.485731Z","shell.execute_reply.started":"2024-06-19T14:17:34.446903Z","shell.execute_reply":"2024-06-19T14:17:37.484838Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n quantize_layer (QuantizeLa  (None, 20)                3         \n yer)                                                            \n                                                                 \n quant_dense_2 (QuantizeWra  (None, 20)                425       \n pperV2)                                                         \n                                                                 \n quant_flatten_2 (QuantizeW  (None, 20)                1         \n rapperV2)                                                       \n                                                                 \n=================================================================\nTotal params: 429 (1.68 KB)\nTrainable params: 420 (1.64 KB)\nNon-trainable params: 9 (36.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a base model\nbase_model = setup_model()\nbase_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n\n# Helper function uses `quantize_annotate_layer` to annotate that only the \n# Dense layers should be quantized.\ndef apply_quantization_to_dense(layer):\n  if isinstance(layer, keras.layers.Dense):\n    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n  return layer\n\n# Use `keras.models.clone_model` to apply `apply_quantization_to_dense` \n# to the layers of the model.\nannotated_model = keras.models.clone_model(\n    base_model,\n    clone_function=apply_quantization_to_dense,\n)\n\n# Now that the Dense layers are annotated,\n# `quantize_apply` actually makes the model quantization aware.\nquant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\nquant_aware_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:18:12.820188Z","iopub.execute_input":"2024-06-19T14:18:12.820866Z","iopub.status.idle":"2024-06-19T14:18:12.994884Z","shell.execute_reply.started":"2024-06-19T14:18:12.820831Z","shell.execute_reply":"2024-06-19T14:18:12.993849Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n quantize_layer_1 (Quantize  (None, 20)                3         \n Layer)                                                          \n                                                                 \n quant_dense_3 (QuantizeWra  (None, 20)                425       \n pperV2)                                                         \n                                                                 \n flatten_3 (Flatten)         (None, 20)                0         \n                                                                 \n=================================================================\nTotal params: 428 (1.67 KB)\nTrainable params: 420 (1.64 KB)\nNon-trainable params: 8 (32.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"print(base_model.layers[0].name)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:18:30.135859Z","iopub.execute_input":"2024-06-19T14:18:30.136383Z","iopub.status.idle":"2024-06-19T14:18:30.142207Z","shell.execute_reply.started":"2024-06-19T14:18:30.136323Z","shell.execute_reply":"2024-06-19T14:18:30.141153Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"dense_3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use `quantize_annotate_layer` to annotate that the `Dense` layer\n# should be quantized.\ni = keras.Input(shape=(20,))\nx = tfmot.quantization.keras.quantize_annotate_layer(keras.layers.Dense(10))(i)\no = keras.layers.Flatten()(x)\nannotated_model = keras.Model(inputs=i, outputs=o)\n\n# Use `quantize_apply` to actually make the model quantization aware.\nquant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n\n# For deployment purposes, the tool adds `QuantizeLayer` after `InputLayer` so that the\n# quantized model can take in float inputs instead of only uint8.\nquant_aware_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:18:45.804055Z","iopub.execute_input":"2024-06-19T14:18:45.804432Z","iopub.status.idle":"2024-06-19T14:18:45.970658Z","shell.execute_reply.started":"2024-06-19T14:18:45.804403Z","shell.execute_reply":"2024-06-19T14:18:45.969764Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 20)]              0         \n                                                                 \n quantize_layer_2 (Quantize  (None, 20)                3         \n Layer)                                                          \n                                                                 \n quant_dense_4 (QuantizeWra  (None, 10)                215       \n pperV2)                                                         \n                                                                 \n flatten_4 (Flatten)         (None, 10)                0         \n                                                                 \n=================================================================\nTotal params: 218 (872.00 Byte)\nTrainable params: 210 (840.00 Byte)\nNon-trainable params: 8 (32.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use `quantize_annotate_layer` to annotate that the `Dense` layer\n# should be quantized.\nannotated_model = keras.Sequential([\n  tfmot.quantization.keras.quantize_annotate_layer(keras.layers.Dense(20, input_shape=input_shape)),\n  keras.layers.Flatten()\n])\n\n# Use `quantize_apply` to actually make the model quantization aware.\nquant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n\nquant_aware_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:19:12.253620Z","iopub.execute_input":"2024-06-19T14:19:12.254521Z","iopub.status.idle":"2024-06-19T14:19:12.411032Z","shell.execute_reply.started":"2024-06-19T14:19:12.254479Z","shell.execute_reply":"2024-06-19T14:19:12.410131Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n quantize_layer_3 (Quantize  (None, 20)                3         \n Layer)                                                          \n                                                                 \n quant_dense_5 (QuantizeWra  (None, 20)                425       \n pperV2)                                                         \n                                                                 \n flatten_5 (Flatten)         (None, 20)                0         \n                                                                 \n=================================================================\nTotal params: 428 (1.67 KB)\nTrainable params: 420 (1.64 KB)\nNon-trainable params: 8 (32.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the model.\nbase_model = setup_model()\nbase_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\nquant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n\n# Save or checkpoint the model.\n_, keras_model_file = tempfile.mkstemp('.h5')\nquant_aware_model.save(keras_model_file)\n\n# `quantize_scope` is needed for deserializing HDF5 models.\nwith tfmot.quantization.keras.quantize_scope():\n  loaded_model = keras.models.load_model(keras_model_file)\n\nloaded_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:19:27.414639Z","iopub.execute_input":"2024-06-19T14:19:27.415019Z","iopub.status.idle":"2024-06-19T14:19:27.683355Z","shell.execute_reply.started":"2024-06-19T14:19:27.414989Z","shell.execute_reply":"2024-06-19T14:19:27.682457Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n quantize_layer_4 (Quantize  (None, 20)                3         \n Layer)                                                          \n                                                                 \n quant_dense_6 (QuantizeWra  (None, 20)                425       \n pperV2)                                                         \n                                                                 \n quant_flatten_6 (QuantizeW  (None, 20)                1         \n rapperV2)                                                       \n                                                                 \n=================================================================\nTotal params: 429 (1.68 KB)\nTrainable params: 420 (1.64 KB)\nNon-trainable params: 9 (36.00 Byte)\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_model_optimization as tfmot\n\n# Setup the pretrained model\nbase_model = setup_pretrained_model()\nquant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n\n# Train the model here\n# (Add your training code here)\n\n# Convert the quantization-aware model to TFLite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_tflite_model = converter.convert()\n\n# Save the converted model to a file\ntflite_model_path = '/kaggle/working/modelML.tflite'\nwith open(tflite_model_path, 'wb') as f:\n    f.write(quantized_tflite_model)\n\n# Copy the TFLite model to the output folder\n#output_model_path = '/kaggle/working/modelML.tflite'\n#output_folder = '/kaggle/working/'\n#import shutil\n#shutil.copy(output_model_path, output_folder)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:36:31.188350Z","iopub.execute_input":"2024-06-19T14:36:31.188790Z","iopub.status.idle":"2024-06-19T14:36:33.471110Z","shell.execute_reply.started":"2024-06-19T14:36:31.188756Z","shell.execute_reply":"2024-06-19T14:36:33.470154Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 847ms/step - loss: 1.3280 - accuracy: 0.0000e+00\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n  warnings.warn(\nSummary on the non-converted ops:\n---------------------------------\n * Accepted dialects: tfl, builtin, func\n * Non-Converted Ops: 1, Total Ops 10, % non-converted = 10.00 %\n * 1 ARITH ops\n\n- arith.constant:    1 occurrences  (i32: 1)\n\n\n\n  (f32: 1)\n  (uq_8: 1)\n  (uq_8: 1, uq_32: 1)\n  (uq_8: 1)\n  (uq_8: 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install TensorFlow.js package\n!pip install tensorflowjs\n\n# Convert the quantization-aware model to TFJS format\nimport tensorflowjs as tfjs\n\ntfjs_model_path = '/kaggle/working/tfjs_model'\ntfjs.converters.save_keras_model(quant_aware_model, tfjs_model_path)\n\n# Copy the TFJS model files to the output folder\nshutil.copytree(tfjs_model_path, output_folder + 'tfjs_model')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:36:39.922866Z","iopub.execute_input":"2024-06-19T14:36:39.923268Z","iopub.status.idle":"2024-06-19T14:36:58.374155Z","shell.execute_reply.started":"2024-06-19T14:36:39.923235Z","shell.execute_reply":"2024-06-19T14:36:58.372900Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting tensorflowjs\n  Downloading tensorflowjs-4.20.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: flax>=0.7.2 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (0.8.4)\nRequirement already satisfied: importlib_resources>=5.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (6.1.1)\nRequirement already satisfied: jax>=0.4.13 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (0.4.26)\nRequirement already satisfied: jaxlib>=0.4.13 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (0.4.26.dev20240504)\nRequirement already satisfied: tensorflow<3,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (2.15.0)\nRequirement already satisfied: tf-keras>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (2.15.1)\nRequirement already satisfied: tensorflow-decision-forests>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (1.8.1)\nRequirement already satisfied: six<2,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (1.16.0)\nRequirement already satisfied: tensorflow-hub>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from tensorflowjs) (0.16.1)\nCollecting packaging~=23.1 (from tensorflowjs)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (1.26.4)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (1.0.7)\nRequirement already satisfied: optax in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\nRequirement already satisfied: orbax-checkpoint in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (0.5.15)\nRequirement already satisfied: tensorstore in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (0.1.60)\nRequirement already satisfied: rich>=11.1 in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (13.7.0)\nRequirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (4.9.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (16.0.6)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (69.0.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.59.3)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.1)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.42.0)\nCollecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.17.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\nRequirement already satisfied: chex>=0.1.86 in /opt/conda/lib/python3.10/site-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\nRequirement already satisfied: etils[epath,epy] in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.5.8)\nINFO: pip is looking at multiple versions of tensorstore to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorstore (from flax>=0.7.2->tensorflowjs)\n  Downloading tensorstore-0.1.62-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.61-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.59-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.58-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.57-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.56-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.55-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nINFO: pip is still looking at multiple versions of tensorstore to determine which version is compatible with other requirements. This could take a while.\n  Downloading tensorstore-0.1.54-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.53-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.52-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.51-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nCollecting orbax-checkpoint (from flax>=0.7.2->tensorflowjs)\n  Downloading orbax_checkpoint-0.5.17-py3-none-any.whl.metadata (1.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting tensorstore (from flax>=0.7.2->tensorflowjs)\n  Downloading tensorstore-0.1.50-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.49-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n  Downloading tensorstore-0.1.48-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n  Downloading tensorstore-0.1.47-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n  Downloading tensorstore-0.1.46-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n  Downloading tensorstore-0.1.45-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\nCollecting orbax-checkpoint (from flax>=0.7.2->tensorflowjs)\n  Downloading orbax_checkpoint-0.5.16-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.15-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.14-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.13-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.12-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.11-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.10-py3-none-any.whl.metadata (1.8 kB)\n  Downloading orbax_checkpoint-0.5.9-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.8-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.7-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.6-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.5-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.4-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.3-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.2-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.5.0-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.4.8-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.4.7-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.4.6-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.4.5-py3-none-any.whl.metadata (1.7 kB)\n  Downloading orbax_checkpoint-0.4.4-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\nRequirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.3.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2024.3.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.17.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.2.2)\nDownloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorstore-0.1.45-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading orbax_checkpoint-0.4.4-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\nInstalling collected packages: wurlitzer, tensorstore, packaging, orbax-checkpoint, tensorflowjs\n  Attempting uninstall: tensorstore\n    Found existing installation: tensorstore 0.1.60\n    Uninstalling tensorstore-0.1.60:\n      Successfully uninstalled tensorstore-0.1.60\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orbax-checkpoint\n    Found existing installation: orbax-checkpoint 0.5.15\n    Uninstalling orbax-checkpoint-0.5.15:\n      Successfully uninstalled orbax-checkpoint-0.5.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed orbax-checkpoint-0.4.4 packaging-23.2 tensorflowjs-4.20.0 tensorstore-0.1.45 wurlitzer-3.1.1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tfjs\u001b[38;5;241m.\u001b[39mconverters\u001b[38;5;241m.\u001b[39msave_keras_model(quant_aware_model, tfjs_model_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Copy the TFJS model files to the output folder\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfjs_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtfjs_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:559\u001b[0m, in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(src) \u001b[38;5;28;01mas\u001b[39;00m itr:\n\u001b[1;32m    558\u001b[0m     entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itr)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_copytree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_dangling_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:457\u001b[0m, in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     ignored_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 457\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirs_exist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    459\u001b[0m use_srcentry \u001b[38;5;241m=\u001b[39m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy2 \u001b[38;5;129;01mor\u001b[39;00m copy_function \u001b[38;5;129;01mis\u001b[39;00m copy\n","File \u001b[0;32m/opt/conda/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n","\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/kaggle/working/tfjs_model'"],"ename":"FileExistsError","evalue":"[Errno 17] File exists: '/kaggle/working/tfjs_model'","output_type":"error"}]},{"cell_type":"code","source":"# Load the existing trained model\n#model_path = '/kaggle/working/model.h5'\n#try:\n    #model = load_model(model_path)\n   # print = (\"Model Succesfully Loaded\")\n    \n    ## Print Model Summary\n    #model.summary()\n#except Exception as e:\n    #print(f\"Error loading the model: {e}\")\n    # Exit the program if loading the model fails\n    #exit()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:15:29.076345Z","iopub.execute_input":"2024-06-19T14:15:29.077002Z","iopub.status.idle":"2024-06-19T14:15:29.083384Z","shell.execute_reply.started":"2024-06-19T14:15:29.076972Z","shell.execute_reply":"2024-06-19T14:15:29.082439Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Error loading the model: [Errno 2] Unable to synchronously open file (unable to open file: name = '/kaggle/working/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Convert the model to TensorFlow Lite format\n#converter = tf.lite.TFLiteConverter.from_saved_model(model)\n#tflite_model = converter.convert()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T14:24:03.012300Z","iopub.execute_input":"2024-06-19T14:24:03.013377Z","iopub.status.idle":"2024-06-19T14:24:03.049076Z","shell.execute_reply.started":"2024-06-19T14:24:03.013341Z","shell.execute_reply":"2024-06-19T14:24:03.047902Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the model to TensorFlow Lite format\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_saved_model(\u001b[43mmodel\u001b[49m)\n\u001b[1;32m      3\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Save the converted model to a file\n#tflite_model_path = '/kaggle/working/model_ML.tflite'\n#with open(tflite_model_path, 'wb') as f:\n    #f.write(tflite_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}